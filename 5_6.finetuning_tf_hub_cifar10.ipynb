{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-beta1\n",
      "2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import ReLU,Dense, BatchNormalization, Softmax, GlobalAveragePooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "training_epochs = 20\n",
    "batch_size = 100\n",
    "img_size = 224"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cifar10 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cifar10 Dataset #########################################################\n",
    "cifar = keras.datasets.cifar10\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = cifar.load_data()    \n",
    "    \n",
    "train_images = train_images.astype(np.float32)/255.\n",
    "test_images = test_images.astype(np.float32)/255.\n",
    "    \n",
    "train_labels = to_categorical(train_labels, 10)\n",
    "test_labels = to_categorical(test_labels, 10)\n",
    "    \n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(\n",
    "                buffer_size=100000).batch(batch_size)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tf_hub에 있는 pretrained model url\n",
    "feature_extractor_url = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## hub.KerasLayer를 이용하여 tf_hub에서 pretrained model loading\n",
    "feature_extractor_layer = hub.KerasLayer(feature_extractor_url, input_shape=(img_size, img_size, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1280)\n"
     ]
    }
   ],
   "source": [
    "## train_dataset에서 1개의 batch만 가져와서 pretrained model에 넣고 output shape을 출력\n",
    "for images, labels in train_dataset.take(1):\n",
    "    images = tf.image.resize(images, (img_size, img_size))\n",
    "    feature_maps = feature_extractor_layer(images)\n",
    "    print(feature_maps.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(feature_extractor_layer)\n",
    "    model.add(Dense(128))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(ReLU())\n",
    "    model.add(Dense(10))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Softmax())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "keras_layer (KerasLayer)     (None, 1280)              2257984   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               163968    \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "softmax (Softmax)            (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 2,423,794\n",
      "Trainable params: 165,534\n",
      "Non-trainable params: 2,258,260\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def loss_fn(model, images, labels):\n",
    "    predictions = model(images, training=True)\n",
    "    loss = tf.reduce_mean(keras.losses.categorical_crossentropy(labels, predictions))   \n",
    "    return loss   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train(model, images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = loss_fn(model, images, labels)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caculating Model's Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def evaluate(model, images, labels):\n",
    "    predictions = model(images, training=False)\n",
    "    correct_prediction = tf.equal(tf.argmax(predictions, 1), tf.argmax(labels, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning started. It takes sometime.\n",
      "Epoch: 1 loss = 0.69330543 train accuracy =  0.8216 test accuracy =  0.8416\n",
      "Epoch: 2 loss = 0.47141683 train accuracy =  0.8741 test accuracy =  0.8522\n",
      "Epoch: 3 loss = 0.37201279 train accuracy =  0.8932 test accuracy =  0.8531\n",
      "Epoch: 4 loss = 0.29823610 train accuracy =  0.9114 test accuracy =  0.8518\n",
      "Epoch: 5 loss = 0.23590051 train accuracy =  0.9267 test accuracy =  0.8494\n",
      "Epoch: 6 loss = 0.18205740 train accuracy =  0.9401 test accuracy =  0.8454\n",
      "Epoch: 7 loss = 0.13634875 train accuracy =  0.9533 test accuracy =  0.8448\n",
      "Epoch: 8 loss = 0.09920695 train accuracy =  0.9644 test accuracy =  0.8409\n",
      "Epoch: 9 loss = 0.07017039 train accuracy =  0.9738 test accuracy =  0.8409\n",
      "Epoch: 10 loss = 0.04906841 train accuracy =  0.9819 test accuracy =  0.8381\n",
      "Epoch: 11 loss = 0.03448673 train accuracy =  0.9873 test accuracy =  0.8404\n",
      "Epoch: 12 loss = 0.02452154 train accuracy =  0.9911 test accuracy =  0.8442\n",
      "Epoch: 13 loss = 0.01796298 train accuracy =  0.9934 test accuracy =  0.8440\n",
      "Epoch: 14 loss = 0.01320712 train accuracy =  0.9944 test accuracy =  0.8458\n",
      "Epoch: 15 loss = 0.00988710 train accuracy =  0.9962 test accuracy =  0.8441\n",
      "Epoch: 16 loss = 0.06202506 train accuracy =  0.9438 test accuracy =  0.8225\n",
      "Epoch: 17 loss = 0.04526040 train accuracy =  0.9673 test accuracy =  0.8451\n",
      "Epoch: 18 loss = 0.01611550 train accuracy =  0.9924 test accuracy =  0.8541\n",
      "Epoch: 19 loss = 0.00905148 train accuracy =  0.9972 test accuracy =  0.8563\n",
      "Epoch: 20 loss = 0.00608325 train accuracy =  0.9987 test accuracy =  0.8544\n",
      "Learning Finished!\n"
     ]
    }
   ],
   "source": [
    "## train my model\n",
    "print('Learning started. It takes sometime.')\n",
    "for epoch in range(training_epochs):\n",
    "    avg_loss = 0.\n",
    "    avg_train_acc = 0.\n",
    "    avg_test_acc = 0.\n",
    "    train_step = 0\n",
    "    test_step = 0\n",
    "    \n",
    "    for images, labels in train_dataset:\n",
    "        images = tf.image.resize(images, (img_size, img_size)) \n",
    "        train(model,images, labels)\n",
    "        loss = loss_fn(model, images, labels)\n",
    "        acc = evaluate(model, images, labels)\n",
    "        avg_loss = avg_loss + loss\n",
    "        avg_train_acc = avg_train_acc + acc\n",
    "        train_step += 1\n",
    "    avg_loss = avg_loss / train_step\n",
    "    avg_train_acc = avg_train_acc / train_step\n",
    "    \n",
    "    for images, labels in test_dataset:\n",
    "        images = tf.image.resize(images, (img_size, img_size))\n",
    "        acc = evaluate(model, images, labels)        \n",
    "        avg_test_acc = avg_test_acc + acc\n",
    "        test_step += 1    \n",
    "    avg_test_acc = avg_test_acc / test_step    \n",
    "\n",
    "    print('Epoch:', '{}'.format(epoch + 1), 'loss =', '{:.8f}'.format(avg_loss), \n",
    "          'train accuracy = ', '{:.4f}'.format(avg_train_acc), \n",
    "          'test accuracy = ', '{:.4f}'.format(avg_test_acc))\n",
    "\n",
    "\n",
    "print('Learning Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
