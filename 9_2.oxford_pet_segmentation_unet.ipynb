{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oxford-IIIT Pet Dataset 을 이용하여 segmentation 학습하기\n",
    "\n",
    "이 실습에서는 미리 저장된 tfrecord file을 읽어와서 unet과 비슷한 network을 이용항 segmentation을 학습시켜 봅니다.\n",
    "\n",
    "### 이 실습 file부터 진행하는 경우 준비사항\n",
    "9_1을 건너뛰고 이번 file부터 실습을 하는 경우 아래 경로에서 tfrecord file을 다운 받은 후,  \n",
    "train tfrecord file:  https://drive.google.com/file/d/1lejq39iRTpQjKH4iZwoL_hCyxzXgoaox/view?usp=sharing  \n",
    "validation tfrecord file: https://drive.google.com/file/d/1qiqEJkcdaHWJUSTX43X-PsihM8PZ6_gt/view?usp=sharing \n",
    "\n",
    "실습 directory에 dogs_tfr 이라는 이름의 directory를 생성하고 위에서 download 받은 file을 upload 해야 실습이 진행 가능합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## library import\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "from tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, Dense, BatchNormalization, GlobalAveragePooling2D, Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd '/content/gdrive/My Drive/TensorFlow_Training_13th'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## hyper parameter 설정\n",
    "batch_size = 10\n",
    "learning_rate = 0.0001\n",
    "training_epochs = 20\n",
    "img_size = 224\n",
    "n_train = 2000\n",
    "n_val = 498"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tfrecord file 경로 설정\n",
    "cur_dir = os.getcwd()\n",
    "tfr_dir = os.path.join(cur_dir, 'dogs_tfr')\n",
    "\n",
    "tfr_train = 'dogs_seg_train.tfrecord'\n",
    "tfr_val = 'dogs_seg_val.tfrecord'\n",
    "\n",
    "tfr_train_dir = os.path.join(tfr_dir, tfr_train)\n",
    "tfr_val_dir = os.path.join(tfr_dir, tfr_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tfrecord file을 data로 parsing해주는 function\n",
    "def _parse_function(tfrecord_serialized):\n",
    "    features={'image': tf.io.FixedLenFeature([], tf.string),\n",
    "              'seg': tf.io.FixedLenFeature([], tf.string),\n",
    "              'grayimg': tf.io.FixedLenFeature([], tf.string)\n",
    "             }\n",
    "    parsed_features = tf.io.parse_single_example(tfrecord_serialized, features)\n",
    "    \n",
    "    image = tf.io.decode_raw(parsed_features['image'], tf.uint8)\n",
    "    image = tf.reshape(image, [img_size, img_size, 3])\n",
    "    image = tf.cast(image, tf.float32)/255.\n",
    "    #image = preprocess_input(image)\n",
    "    \n",
    "    seg = tf.io.decode_raw(parsed_features['seg'], tf.uint8)\n",
    "    seg = tf.reshape(seg, [img_size, img_size, -1])\n",
    "    seg = tf.cast(seg, tf.float32)\n",
    "    \n",
    "    grayimg = tf.io.decode_raw(parsed_features['grayimg'], tf.float64)    \n",
    "    grayimg = tf.reshape(grayimg, [img_size, img_size])\n",
    "    grayimg = tf.stack([grayimg, grayimg, grayimg], -1)\n",
    "    grayimg = tf.cast(grayimg, tf.float32)/255.\n",
    "    #print(grayimg.shape)\n",
    "    \n",
    "    return image, seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train dataset 만들기\n",
    "train_dataset = tf.data.TFRecordDataset(tfr_train_dir)\n",
    "train_dataset = train_dataset.map(_parse_function, num_parallel_calls=8)\n",
    "#train_dataset = train_dataset.prefetch(buffer_size=batch_size).batch(batch_size).repeat()\n",
    "train_dataset = train_dataset.prefetch(buffer_size=batch_size).batch(batch_size).shuffle(n_train*2).repeat()\n",
    "train_dataset = train_dataset.apply(tf.data.experimental.ignore_errors())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## validation datset 만들기\n",
    "val_dataset = tf.data.TFRecordDataset(tfr_val_dir)\n",
    "val_dataset = val_dataset.map(_parse_function, num_parallel_calls=8)\n",
    "val_dataset = val_dataset.prefetch(buffer_size=batch_size).batch(batch_size)\n",
    "val_dataset = val_dataset.apply(tf.data.experimental.ignore_errors())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train dataset에서 1개의 image와 segmentation label을 읽어서 확인\n",
    "for image, seg in train_dataset.take(1):\n",
    "    plt.figure(figsize=(11, 5))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(image[0])\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(seg[0,:,:,0], vmin=0, vmax=1) \n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    inputs = Input(shape=(img_size, img_size, 3))\n",
    "    \n",
    "    conv1_1 = Conv2D(64, 3, 1, 'SAME', activation='relu')(inputs)\n",
    "    conv1_2 = Conv2D(64, 3, 1, 'SAME', activation='relu')(conv1_1)\n",
    "    pool1_3 = MaxPooling2D()(conv1_2)\n",
    "    \n",
    "    conv2_1 = Conv2D(128, 3, 1, 'SAME', activation='relu')(pool1_3)\n",
    "    conv2_2 = Conv2D(128, 3, 1, 'SAME', activation='relu')(conv2_1)\n",
    "    pool2_3 = MaxPooling2D()(conv2_2)\n",
    "    \n",
    "    conv3_1 = Conv2D(256, 3, 1, 'SAME', activation='relu')(pool2_3)\n",
    "    conv3_2 = Conv2D(256, 3, 1, 'SAME', activation='relu')(conv3_1)\n",
    "    conv3_3 = Conv2D(256, 3, 1, 'SAME', activation='relu')(conv3_2)\n",
    "    pool3_4 = MaxPooling2D()(conv3_3)\n",
    "    \n",
    "    conv4_1 = Conv2D(512, 3, 1, 'SAME', activation='relu')(pool3_4)\n",
    "    conv4_2 = Conv2D(512, 3, 1, 'SAME', activation='relu')(conv4_1)\n",
    "    conv4_3 = Conv2D(512, 3, 1, 'SAME', activation='relu')(conv4_2)\n",
    "    pool4_4 = MaxPooling2D()(conv4_3)\n",
    "    \n",
    "    conv5_1 = Conv2D(512, 3, 1, 'SAME', activation='relu')(pool4_4)\n",
    "    conv5_2 = Conv2D(512, 3, 1, 'SAME', activation='relu')(conv5_1)\n",
    "    conv5_3 = Conv2D(512, 3, 1, 'SAME', activation='relu')(conv5_2)\n",
    "    pool5_4 = MaxPooling2D()(conv5_3)\n",
    "    \n",
    "    upconv6 = Conv2DTranspose(512, 5, 2, 'SAME', activation='relu')(pool5_4)\n",
    "    concat6 = Concatenate()([conv5_3, upconv6])\n",
    "    conv6 = Conv2D(512, 3, 1, 'SAME', activation='relu')(concat6)\n",
    "                              \n",
    "    upconv7 = Conv2DTranspose(512, 5, 2, 'SAME', activation='relu')(conv6)\n",
    "    concat7 = Concatenate()([conv4_3, upconv7])\n",
    "    conv7 = Conv2D(512, 3, 1, 'SAME', activation='relu')(concat7)\n",
    "    \n",
    "    upconv8 = Conv2DTranspose(256, 5, 2, 'SAME', activation='relu')(conv7)\n",
    "    concat8 = Concatenate()([conv3_3, upconv8])\n",
    "    conv8 = Conv2D(256, 3, 1, 'SAME', activation='relu')(concat8)\n",
    "    \n",
    "    upconv9 = Conv2DTranspose(128, 5, 2, 'SAME', activation='relu')(conv8)\n",
    "    concat9 = Concatenate()([conv2_2, upconv9])\n",
    "    conv9 = Conv2D(128, 3, 1, 'SAME', activation='relu')(concat9)\n",
    "    \n",
    "    upconv10 = Conv2DTranspose(64, 5, 2, 'SAME', activation='relu')(conv9)\n",
    "    concat10 = Concatenate()([conv1_2, upconv10])\n",
    "    conv10 = Conv2D(64, 3, 1, 'SAME', activation='relu')(concat10)\n",
    "    \n",
    "    conv11 = Conv2D(64, 3, 1, 'SAME', activation='relu')(conv10)\n",
    "    \n",
    "    conv12 = Conv2D(2, 1, 1, 'SAME', activation='softmax')(conv11)\n",
    "    \n",
    "    return keras.Model(inputs=inputs, outputs=conv12)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## learning rate scheduing\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=learning_rate,\n",
    "                                                          decay_steps=n_train//batch_size*10,\n",
    "                                                          decay_rate=0.4,\n",
    "                                                          staircase=True)\n",
    "## optimizer는 Adam, loss는 sparse categorical crossentropy 사용\n",
    "## label이 ont-hot으로 encoding 안 된 경우에 sparse categorical corssentropy 및 sparse categorical accuracy 사용\n",
    "model.compile(optimizers.Adam(lr_schedule), loss='sparse_categorical_crossentropy',\n",
    "             metrics=['sparse_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train!\n",
    "steps_per_epoch = n_train//batch_size\n",
    "validation_steps = n_val//batch_size\n",
    "model.fit(train_dataset, steps_per_epoch=steps_per_epoch,\n",
    "         epochs=training_epochs,\n",
    "         validation_data=val_dataset,\n",
    "         validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pretrained weight load(optinal)\n",
    "미리 학습된 weight를 가져와서 학습할 경우 아래 comment를 지우고 load_weights를 실행  \n",
    "미리 학습된 weight 위치: https://drive.google.com/file/d/1-hSnPO4s2OVpsbdjMSaHiv9U2AFb0z05/view?usp=sharing  \n",
    "위 link에서 .h5 파일을 download한 후에 실습 directory에 'saved_weights'라는 directory를 만든 후 .h5 파일을 upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weight_path = ('./saved_weights/segmentation_vgg.h5')\n",
    "#model.load_weights(weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## validation data에 대한 예측값 생성\n",
    "prediction = model.predict(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.zeros_like(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 0.5이상은 1로 나머지는 0으로 변환\n",
    "thr = 0.5\n",
    "pred[prediction>=thr] = 1\n",
    "pred[prediction<thr] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## num_imgs만큼 validation dataset에서 읽어서 정답과 예측값 확인\n",
    "num_imgs = 10\n",
    "idx = 0\n",
    "for image, seg in val_dataset.take(num_imgs):\n",
    "    plt.figure(figsize=(17, 6*num_imgs))\n",
    "    plt.subplot(num_imgs,3,idx*3+1)\n",
    "    plt.imshow(image[0])\n",
    "    plt.subplot(num_imgs,3,idx*3+2)\n",
    "    plt.imshow(seg[0,:,:,0], vmin=0, vmax=1)\n",
    "    plt.subplot(num_imgs,3,idx*3+3)\n",
    "    plt.imshow(pred[idx*batch_size,:,:,1])\n",
    "    plt.show() \n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
