{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Language Model with RNN\n",
    "\n",
    "RNN을 이용하여 간단한 language model을 학습시켜 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-beta1\n",
      "2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "## library import\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 학습시킬 문장\n",
    "sentence = (\"if you want to build a ship, don't drum up people together to \"\n",
    "            \"collect wood and don't assign them tasks and work, but rather \"\n",
    "            \"teach them to long for the endless immensity of the sea.\")\n",
    "\n",
    "## index를 주면 charcter로 바꿔주는 list\n",
    "idx2char = list(set(sentence))\n",
    "## character를 주면 index로 바꿔주는 dictionary\n",
    "char2idx = {w: i for i, w in enumerate(idx2char)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['n',\n",
       " 'u',\n",
       " 'i',\n",
       " 'h',\n",
       " ' ',\n",
       " 'w',\n",
       " '.',\n",
       " 'm',\n",
       " 'b',\n",
       " ',',\n",
       " 'o',\n",
       " 'e',\n",
       " 'c',\n",
       " 's',\n",
       " 'f',\n",
       " 'l',\n",
       " 'r',\n",
       " 't',\n",
       " 'y',\n",
       " 'a',\n",
       " 'p',\n",
       " 'k',\n",
       " 'g',\n",
       " 'd',\n",
       " \"'\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n': 0,\n",
       " 'u': 1,\n",
       " 'i': 2,\n",
       " 'h': 3,\n",
       " ' ': 4,\n",
       " 'w': 5,\n",
       " '.': 6,\n",
       " 'm': 7,\n",
       " 'b': 8,\n",
       " ',': 9,\n",
       " 'o': 10,\n",
       " 'e': 11,\n",
       " 'c': 12,\n",
       " 's': 13,\n",
       " 'f': 14,\n",
       " 'l': 15,\n",
       " 'r': 16,\n",
       " 't': 17,\n",
       " 'y': 18,\n",
       " 'a': 19,\n",
       " 'p': 20,\n",
       " 'k': 21,\n",
       " 'g': 22,\n",
       " 'd': 23,\n",
       " \"'\": 24}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "data_dim = len(idx2char)\n",
    "hidden_size = len(idx2char)\n",
    "num_classes = len(idx2char)\n",
    "sequence_length = 10  # Any arbitrary number\n",
    "learning_rate = 0.1\n",
    "training_epochs = 500\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 if you wan -> f you want\n",
      "1 f you want ->  you want \n",
      "2  you want  -> you want t\n",
      "3 you want t -> ou want to\n",
      "4 ou want to -> u want to \n",
      "5 u want to  ->  want to b\n",
      "6  want to b -> want to bu\n",
      "7 want to bu -> ant to bui\n",
      "8 ant to bui -> nt to buil\n",
      "9 nt to buil -> t to build\n",
      "10 t to build ->  to build \n",
      "11  to build  -> to build a\n",
      "12 to build a -> o build a \n",
      "13 o build a  ->  build a s\n",
      "14  build a s -> build a sh\n",
      "15 build a sh -> uild a shi\n",
      "16 uild a shi -> ild a ship\n",
      "17 ild a ship -> ld a ship,\n",
      "18 ld a ship, -> d a ship, \n",
      "19 d a ship,  ->  a ship, d\n",
      "20  a ship, d -> a ship, do\n",
      "21 a ship, do ->  ship, don\n",
      "22  ship, don -> ship, don'\n",
      "23 ship, don' -> hip, don't\n",
      "24 hip, don't -> ip, don't \n",
      "25 ip, don't  -> p, don't d\n",
      "26 p, don't d -> , don't dr\n",
      "27 , don't dr ->  don't dru\n",
      "28  don't dru -> don't drum\n",
      "29 don't drum -> on't drum \n",
      "30 on't drum  -> n't drum u\n",
      "31 n't drum u -> 't drum up\n",
      "32 't drum up -> t drum up \n",
      "33 t drum up  ->  drum up p\n",
      "34  drum up p -> drum up pe\n",
      "35 drum up pe -> rum up peo\n",
      "36 rum up peo -> um up peop\n",
      "37 um up peop -> m up peopl\n",
      "38 m up peopl ->  up people\n",
      "39  up people -> up people \n",
      "40 up people  -> p people t\n",
      "41 p people t ->  people to\n",
      "42  people to -> people tog\n",
      "43 people tog -> eople toge\n",
      "44 eople toge -> ople toget\n",
      "45 ople toget -> ple togeth\n",
      "46 ple togeth -> le togethe\n",
      "47 le togethe -> e together\n",
      "48 e together ->  together \n",
      "49  together  -> together t\n",
      "50 together t -> ogether to\n",
      "51 ogether to -> gether to \n",
      "52 gether to  -> ether to c\n",
      "53 ether to c -> ther to co\n",
      "54 ther to co -> her to col\n",
      "55 her to col -> er to coll\n",
      "56 er to coll -> r to colle\n",
      "57 r to colle ->  to collec\n",
      "58  to collec -> to collect\n",
      "59 to collect -> o collect \n",
      "60 o collect  ->  collect w\n",
      "61  collect w -> collect wo\n",
      "62 collect wo -> ollect woo\n",
      "63 ollect woo -> llect wood\n",
      "64 llect wood -> lect wood \n",
      "65 lect wood  -> ect wood a\n",
      "66 ect wood a -> ct wood an\n",
      "67 ct wood an -> t wood and\n",
      "68 t wood and ->  wood and \n",
      "69  wood and  -> wood and d\n",
      "70 wood and d -> ood and do\n",
      "71 ood and do -> od and don\n",
      "72 od and don -> d and don'\n",
      "73 d and don' ->  and don't\n",
      "74  and don't -> and don't \n",
      "75 and don't  -> nd don't a\n",
      "76 nd don't a -> d don't as\n",
      "77 d don't as ->  don't ass\n",
      "78  don't ass -> don't assi\n",
      "79 don't assi -> on't assig\n",
      "80 on't assig -> n't assign\n",
      "81 n't assign -> 't assign \n",
      "82 't assign  -> t assign t\n",
      "83 t assign t ->  assign th\n",
      "84  assign th -> assign the\n",
      "85 assign the -> ssign them\n",
      "86 ssign them -> sign them \n",
      "87 sign them  -> ign them t\n",
      "88 ign them t -> gn them ta\n",
      "89 gn them ta -> n them tas\n",
      "90 n them tas ->  them task\n",
      "91  them task -> them tasks\n",
      "92 them tasks -> hem tasks \n",
      "93 hem tasks  -> em tasks a\n",
      "94 em tasks a -> m tasks an\n",
      "95 m tasks an ->  tasks and\n",
      "96  tasks and -> tasks and \n",
      "97 tasks and  -> asks and w\n",
      "98 asks and w -> sks and wo\n",
      "99 sks and wo -> ks and wor\n",
      "100 ks and wor -> s and work\n",
      "101 s and work ->  and work,\n",
      "102  and work, -> and work, \n",
      "103 and work,  -> nd work, b\n",
      "104 nd work, b -> d work, bu\n",
      "105 d work, bu ->  work, but\n",
      "106  work, but -> work, but \n",
      "107 work, but  -> ork, but r\n",
      "108 ork, but r -> rk, but ra\n",
      "109 rk, but ra -> k, but rat\n",
      "110 k, but rat -> , but rath\n",
      "111 , but rath ->  but rathe\n",
      "112  but rathe -> but rather\n",
      "113 but rather -> ut rather \n",
      "114 ut rather  -> t rather t\n",
      "115 t rather t ->  rather te\n",
      "116  rather te -> rather tea\n",
      "117 rather tea -> ather teac\n",
      "118 ather teac -> ther teach\n",
      "119 ther teach -> her teach \n",
      "120 her teach  -> er teach t\n",
      "121 er teach t -> r teach th\n",
      "122 r teach th ->  teach the\n",
      "123  teach the -> teach them\n",
      "124 teach them -> each them \n",
      "125 each them  -> ach them t\n",
      "126 ach them t -> ch them to\n",
      "127 ch them to -> h them to \n",
      "128 h them to  ->  them to l\n",
      "129  them to l -> them to lo\n",
      "130 them to lo -> hem to lon\n",
      "131 hem to lon -> em to long\n",
      "132 em to long -> m to long \n",
      "133 m to long  ->  to long f\n",
      "134  to long f -> to long fo\n",
      "135 to long fo -> o long for\n",
      "136 o long for ->  long for \n",
      "137  long for  -> long for t\n",
      "138 long for t -> ong for th\n",
      "139 ong for th -> ng for the\n",
      "140 ng for the -> g for the \n",
      "141 g for the  ->  for the e\n",
      "142  for the e -> for the en\n",
      "143 for the en -> or the end\n",
      "144 or the end -> r the endl\n",
      "145 r the endl ->  the endle\n",
      "146  the endle -> the endles\n",
      "147 the endles -> he endless\n",
      "148 he endless -> e endless \n",
      "149 e endless  ->  endless i\n",
      "150  endless i -> endless im\n",
      "151 endless im -> ndless imm\n",
      "152 ndless imm -> dless imme\n",
      "153 dless imme -> less immen\n",
      "154 less immen -> ess immens\n",
      "155 ess immens -> ss immensi\n",
      "156 ss immensi -> s immensit\n",
      "157 s immensit ->  immensity\n",
      "158  immensity -> immensity \n",
      "159 immensity  -> mmensity o\n",
      "160 mmensity o -> mensity of\n",
      "161 mensity of -> ensity of \n",
      "162 ensity of  -> nsity of t\n",
      "163 nsity of t -> sity of th\n",
      "164 sity of th -> ity of the\n",
      "165 ity of the -> ty of the \n",
      "166 ty of the  -> y of the s\n",
      "167 y of the s ->  of the se\n",
      "168  of the se -> of the sea\n",
      "169 of the sea -> f the sea.\n"
     ]
    }
   ],
   "source": [
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, len(sentence) - sequence_length):\n",
    "    x_str = sentence[i:i + sequence_length]\n",
    "    y_str = sentence[i + 1: i + sequence_length + 1]\n",
    "    print(i, x_str, '->', y_str)\n",
    "\n",
    "    x = [char2idx[c] for c in x_str]  # x str to index\n",
    "    y = [char2idx[c] for c in y_str]  # y str to index\n",
    "\n",
    "    dataX.append(x)\n",
    "    dataY.append(y)\n",
    "\n",
    "batch_size = len(dataX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(170, 10, 25) (170, 10, 25)\n",
      "170\n"
     ]
    }
   ],
   "source": [
    "dataX = np.array(to_categorical(dataX, num_classes))\n",
    "dataY = np.array(to_categorical(dataY, num_classes))\n",
    "print(dataX.shape, dataY.shape)\n",
    "print(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((dataX, dataY)).shuffle(\n",
    "                buffer_size=1000).prefetch(buffer_size=batch_size).batch(batch_size).repeat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.LSTM(units=hidden_size, return_sequences=True,\n",
    "                                     input_shape=(dataX.shape[1],dataX.shape[2])))\n",
    "    model.add(layers.LSTM(units=hidden_size, return_sequences=True))\n",
    "    model.add(layers.Dense(units=num_classes, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 10, 25)            5100      \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 10, 25)            5100      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10, 25)            650       \n",
      "=================================================================\n",
      "Total params: 10,850\n",
      "Trainable params: 10,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return keras.losses.categorical_crossentropy(labels, logits)\n",
    "\n",
    "def adam_opt(learning_rate):\n",
    "    return keras.optimizers.Adam(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=adam_opt(learning_rate),\n",
    "              loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0824 02:45:28.669958 11096 deprecation.py:323] From C:\\Users\\jwlee\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "1/1 [==============================] - 2s 2s/step - loss: 3.2188\n",
      "Epoch 2/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 3.0053\n",
      "Epoch 3/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.2812\n",
      "Epoch 4/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.9679\n",
      "Epoch 5/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.8929\n",
      "Epoch 6/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.9012\n",
      "Epoch 7/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.8685\n",
      "Epoch 8/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.8389\n",
      "Epoch 9/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.8597\n",
      "Epoch 10/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.7804\n",
      "Epoch 11/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.7606\n",
      "Epoch 12/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.7140\n",
      "Epoch 13/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.6654\n",
      "Epoch 14/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.6143\n",
      "Epoch 15/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.5593\n",
      "Epoch 16/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.4746\n",
      "Epoch 17/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.3849\n",
      "Epoch 18/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.2818\n",
      "Epoch 19/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.1781\n",
      "Epoch 20/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.0706\n",
      "Epoch 21/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.9631\n",
      "Epoch 22/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.8383\n",
      "Epoch 23/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.7246\n",
      "Epoch 24/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.6017\n",
      "Epoch 25/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.4963\n",
      "Epoch 26/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.4074\n",
      "Epoch 27/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.3209\n",
      "Epoch 28/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.2245\n",
      "Epoch 29/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.1567\n",
      "Epoch 30/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.0632\n",
      "Epoch 31/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.0105\n",
      "Epoch 32/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.9262\n",
      "Epoch 33/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.8700\n",
      "Epoch 34/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.8216\n",
      "Epoch 35/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7589\n",
      "Epoch 36/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.6912\n",
      "Epoch 37/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6404\n",
      "Epoch 38/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.5913\n",
      "Epoch 39/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.5581\n",
      "Epoch 40/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5186\n",
      "Epoch 41/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4849\n",
      "Epoch 42/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.4642\n",
      "Epoch 43/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4382\n",
      "Epoch 44/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.4250\n",
      "Epoch 45/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.4037\n",
      "Epoch 46/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.3899\n",
      "Epoch 47/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.3775\n",
      "Epoch 48/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.3670\n",
      "Epoch 49/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.3558\n",
      "Epoch 50/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.3470\n",
      "Epoch 51/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.3394\n",
      "Epoch 52/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.3330\n",
      "Epoch 53/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.3256\n",
      "Epoch 54/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.3195\n",
      "Epoch 55/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.3140\n",
      "Epoch 56/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.3084\n",
      "Epoch 57/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.3036\n",
      "Epoch 58/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2983\n",
      "Epoch 59/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2940\n",
      "Epoch 60/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2899\n",
      "Epoch 61/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2861\n",
      "Epoch 62/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2831\n",
      "Epoch 63/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.2798\n",
      "Epoch 64/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2766\n",
      "Epoch 65/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2743\n",
      "Epoch 66/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2713\n",
      "Epoch 67/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2692\n",
      "Epoch 68/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2668\n",
      "Epoch 69/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2647\n",
      "Epoch 70/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2627\n",
      "Epoch 71/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2614\n",
      "Epoch 72/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2627\n",
      "Epoch 73/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2593\n",
      "Epoch 74/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2573\n",
      "Epoch 75/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2558\n",
      "Epoch 76/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2544\n",
      "Epoch 77/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2529\n",
      "Epoch 78/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2521\n",
      "Epoch 79/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2507\n",
      "Epoch 80/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2497\n",
      "Epoch 81/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2489\n",
      "Epoch 82/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2476\n",
      "Epoch 83/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2472\n",
      "Epoch 84/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2462\n",
      "Epoch 85/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2454\n",
      "Epoch 86/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2450\n",
      "Epoch 87/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2445\n",
      "Epoch 88/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2444\n",
      "Epoch 89/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2452\n",
      "Epoch 90/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2429\n",
      "Epoch 91/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2430\n",
      "Epoch 92/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2423\n",
      "Epoch 93/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2419\n",
      "Epoch 94/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2416\n",
      "Epoch 95/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2412\n",
      "Epoch 96/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2405\n",
      "Epoch 97/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2402\n",
      "Epoch 98/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.2397\n",
      "Epoch 99/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.2394\n",
      "Epoch 100/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2391\n",
      "Epoch 101/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2389\n",
      "Epoch 102/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2384\n",
      "Epoch 103/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2383\n",
      "Epoch 104/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2375\n",
      "Epoch 106/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2376\n",
      "Epoch 107/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2372\n",
      "Epoch 108/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2369\n",
      "Epoch 109/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2369\n",
      "Epoch 110/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2365\n",
      "Epoch 111/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2364\n",
      "Epoch 112/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2363\n",
      "Epoch 113/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2360\n",
      "Epoch 114/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2358\n",
      "Epoch 115/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2355\n",
      "Epoch 116/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2354\n",
      "Epoch 117/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2353\n",
      "Epoch 118/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2350\n",
      "Epoch 119/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2349\n",
      "Epoch 120/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2347\n",
      "Epoch 121/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2345\n",
      "Epoch 122/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.2345\n",
      "Epoch 123/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2343\n",
      "Epoch 124/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2342\n",
      "Epoch 125/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2340\n",
      "Epoch 126/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2339\n",
      "Epoch 127/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2339\n",
      "Epoch 128/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2339\n",
      "Epoch 129/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2339\n",
      "Epoch 130/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2340\n",
      "Epoch 131/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2340\n",
      "Epoch 132/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2340\n",
      "Epoch 133/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2342\n",
      "Epoch 134/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2339\n",
      "Epoch 135/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2332\n",
      "Epoch 136/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2333\n",
      "Epoch 137/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2335\n",
      "Epoch 138/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2334\n",
      "Epoch 139/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2331\n",
      "Epoch 140/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2329\n",
      "Epoch 141/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2329\n",
      "Epoch 142/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2329\n",
      "Epoch 143/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2328\n",
      "Epoch 144/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2325\n",
      "Epoch 145/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2325\n",
      "Epoch 146/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2325\n",
      "Epoch 147/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2325\n",
      "Epoch 148/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2323\n",
      "Epoch 149/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2322\n",
      "Epoch 150/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2321\n",
      "Epoch 151/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2321\n",
      "Epoch 152/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2321\n",
      "Epoch 153/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2321\n",
      "Epoch 154/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2320\n",
      "Epoch 155/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2322\n",
      "Epoch 156/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2320\n",
      "Epoch 157/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2319\n",
      "Epoch 158/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2318\n",
      "Epoch 159/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.2317\n",
      "Epoch 160/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2317\n",
      "Epoch 161/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2317\n",
      "Epoch 162/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2316\n",
      "Epoch 163/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2316\n",
      "Epoch 164/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2315\n",
      "Epoch 165/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2315\n",
      "Epoch 166/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2315\n",
      "Epoch 167/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2316\n",
      "Epoch 168/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2318\n",
      "Epoch 169/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2319\n",
      "Epoch 170/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2317\n",
      "Epoch 171/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2314\n",
      "Epoch 172/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2315\n",
      "Epoch 173/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2315\n",
      "Epoch 174/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2313\n",
      "Epoch 175/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2313\n",
      "Epoch 176/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2313\n",
      "Epoch 177/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2311\n",
      "Epoch 178/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2311\n",
      "Epoch 179/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2312\n",
      "Epoch 180/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2310\n",
      "Epoch 181/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2310\n",
      "Epoch 182/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2310\n",
      "Epoch 183/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2309\n",
      "Epoch 184/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2309\n",
      "Epoch 185/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2309\n",
      "Epoch 186/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2309\n",
      "Epoch 187/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2309\n",
      "Epoch 188/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2310\n",
      "Epoch 189/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2311\n",
      "Epoch 190/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2315\n",
      "Epoch 191/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2319\n",
      "Epoch 192/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2321\n",
      "Epoch 193/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2317\n",
      "Epoch 194/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2312\n",
      "Epoch 195/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.2309\n",
      "Epoch 196/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2309\n",
      "Epoch 197/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2310\n",
      "Epoch 198/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2310\n",
      "Epoch 199/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2309\n",
      "Epoch 200/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2307\n",
      "Epoch 201/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2307\n",
      "Epoch 202/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2308\n",
      "Epoch 203/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2308\n",
      "Epoch 204/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2306\n",
      "Epoch 205/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2305\n",
      "Epoch 206/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2306\n",
      "Epoch 207/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 208/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2305\n",
      "Epoch 209/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2305\n",
      "Epoch 210/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2304\n",
      "Epoch 211/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2305\n",
      "Epoch 212/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2305\n",
      "Epoch 213/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2304\n",
      "Epoch 214/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2303\n",
      "Epoch 215/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2303\n",
      "Epoch 216/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2303\n",
      "Epoch 217/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2303\n",
      "Epoch 218/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2303\n",
      "Epoch 219/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2303\n",
      "Epoch 220/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2302\n",
      "Epoch 221/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2302\n",
      "Epoch 222/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2302\n",
      "Epoch 223/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.2302\n",
      "Epoch 224/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2302\n",
      "Epoch 225/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2302\n",
      "Epoch 226/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2302\n",
      "Epoch 227/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2303\n",
      "Epoch 228/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2303\n",
      "Epoch 229/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2306\n",
      "Epoch 230/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2304\n",
      "Epoch 231/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2304\n",
      "Epoch 232/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2302\n",
      "Epoch 233/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2301\n",
      "Epoch 234/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.2301\n",
      "Epoch 235/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2302\n",
      "Epoch 236/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2303\n",
      "Epoch 237/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2305\n",
      "Epoch 238/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2308\n",
      "Epoch 239/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2307\n",
      "Epoch 240/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2305\n",
      "Epoch 241/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2302\n",
      "Epoch 242/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2302\n",
      "Epoch 243/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2303\n",
      "Epoch 244/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2303\n",
      "Epoch 245/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2302\n",
      "Epoch 246/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2301\n",
      "Epoch 247/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2301\n",
      "Epoch 248/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2301\n",
      "Epoch 249/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2300\n",
      "Epoch 250/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2299\n",
      "Epoch 251/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2299\n",
      "Epoch 252/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2299\n",
      "Epoch 253/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2299\n",
      "Epoch 254/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2299\n",
      "Epoch 255/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2299\n",
      "Epoch 256/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2299\n",
      "Epoch 257/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2299\n",
      "Epoch 258/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2299\n",
      "Epoch 259/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2299\n",
      "Epoch 260/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2298\n",
      "Epoch 261/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2298\n",
      "Epoch 262/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2298\n",
      "Epoch 263/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2298\n",
      "Epoch 264/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2298\n",
      "Epoch 265/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2298\n",
      "Epoch 266/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2298\n",
      "Epoch 267/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2298\n",
      "Epoch 268/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2299\n",
      "Epoch 269/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2300\n",
      "Epoch 270/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2303\n",
      "Epoch 271/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2307\n",
      "Epoch 272/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2309\n",
      "Epoch 273/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2309\n",
      "Epoch 274/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2303\n",
      "Epoch 275/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2298\n",
      "Epoch 276/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2297\n",
      "Epoch 277/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2300\n",
      "Epoch 278/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2302\n",
      "Epoch 279/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2301\n",
      "Epoch 280/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2297\n",
      "Epoch 281/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2297\n",
      "Epoch 282/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2299\n",
      "Epoch 283/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2300\n",
      "Epoch 284/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2298\n",
      "Epoch 285/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2297\n",
      "Epoch 286/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2298\n",
      "Epoch 287/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2298\n",
      "Epoch 288/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2297\n",
      "Epoch 289/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2296\n",
      "Epoch 290/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.2297\n",
      "Epoch 291/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2297\n",
      "Epoch 292/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2296\n",
      "Epoch 293/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2296\n",
      "Epoch 294/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2296\n",
      "Epoch 295/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2296\n",
      "Epoch 296/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2296\n",
      "Epoch 297/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2296\n",
      "Epoch 298/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2296\n",
      "Epoch 299/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2296\n",
      "Epoch 300/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2296\n",
      "Epoch 301/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2295\n",
      "Epoch 302/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2295\n",
      "Epoch 303/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2295\n",
      "Epoch 304/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2295\n",
      "Epoch 305/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2295\n",
      "Epoch 306/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2295\n",
      "Epoch 307/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2295\n",
      "Epoch 308/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2295\n",
      "Epoch 309/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2295\n",
      "Epoch 310/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 311/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2294\n",
      "Epoch 312/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2294\n",
      "Epoch 313/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2294\n",
      "Epoch 314/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2295\n",
      "Epoch 315/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2295\n",
      "Epoch 316/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2295\n",
      "Epoch 317/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2295\n",
      "Epoch 318/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2296\n",
      "Epoch 319/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2297\n",
      "Epoch 320/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2298\n",
      "Epoch 321/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2300\n",
      "Epoch 322/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2299\n",
      "Epoch 323/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2298\n",
      "Epoch 324/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2296\n",
      "Epoch 325/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2296\n",
      "Epoch 326/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2296\n",
      "Epoch 327/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2297\n",
      "Epoch 328/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2298\n",
      "Epoch 329/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2299\n",
      "Epoch 330/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2300\n",
      "Epoch 331/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2300\n",
      "Epoch 332/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.2297\n",
      "Epoch 333/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2294\n",
      "Epoch 334/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2295\n",
      "Epoch 335/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.2296\n",
      "Epoch 336/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2296\n",
      "Epoch 337/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2295\n",
      "Epoch 338/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2294\n",
      "Epoch 339/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2294\n",
      "Epoch 340/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2295\n",
      "Epoch 341/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2294\n",
      "Epoch 342/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2294\n",
      "Epoch 343/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2294\n",
      "Epoch 344/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2294\n",
      "Epoch 345/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2293\n",
      "Epoch 346/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2293\n",
      "Epoch 347/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2294\n",
      "Epoch 348/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2293\n",
      "Epoch 349/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2293\n",
      "Epoch 350/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.2293\n",
      "Epoch 351/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2293\n",
      "Epoch 352/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2293\n",
      "Epoch 353/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2293\n",
      "Epoch 354/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2293\n",
      "Epoch 355/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2293\n",
      "Epoch 356/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2293\n",
      "Epoch 357/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2293\n",
      "Epoch 358/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2293\n",
      "Epoch 359/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2293\n",
      "Epoch 360/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2292\n",
      "Epoch 361/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2292\n",
      "Epoch 362/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2293\n",
      "Epoch 363/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2293\n",
      "Epoch 364/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2293\n",
      "Epoch 365/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2293\n",
      "Epoch 366/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2293\n",
      "Epoch 367/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2294\n",
      "Epoch 368/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2295\n",
      "Epoch 369/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2297\n",
      "Epoch 370/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2300\n",
      "Epoch 371/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2304\n",
      "Epoch 372/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.2305\n",
      "Epoch 373/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2300\n",
      "Epoch 374/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2295\n",
      "Epoch 375/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.2293\n",
      "Epoch 376/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2295\n",
      "Epoch 377/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2296\n",
      "Epoch 378/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2294\n",
      "Epoch 379/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2294\n",
      "Epoch 380/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2295\n",
      "Epoch 381/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2295\n",
      "Epoch 382/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2294\n",
      "Epoch 383/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2293\n",
      "Epoch 384/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2293\n",
      "Epoch 385/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2293\n",
      "Epoch 386/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2294\n",
      "Epoch 387/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2293\n",
      "Epoch 388/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2293\n",
      "Epoch 389/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2293\n",
      "Epoch 390/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2293\n",
      "Epoch 391/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2292\n",
      "Epoch 392/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.2292\n",
      "Epoch 393/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2293\n",
      "Epoch 394/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2292\n",
      "Epoch 395/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2292\n",
      "Epoch 396/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2292\n",
      "Epoch 397/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2292\n",
      "Epoch 398/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2292\n",
      "Epoch 399/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2292\n",
      "Epoch 400/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2292\n",
      "Epoch 401/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.2292\n",
      "Epoch 402/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2291\n",
      "Epoch 403/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2291\n",
      "Epoch 404/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2291\n",
      "Epoch 405/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2291\n",
      "Epoch 406/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2291\n",
      "Epoch 407/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2291\n",
      "Epoch 408/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2291\n",
      "Epoch 409/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2291\n",
      "Epoch 410/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2291\n",
      "Epoch 411/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2291\n",
      "Epoch 412/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2291\n",
      "Epoch 413/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 414/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2291\n",
      "Epoch 415/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2291\n",
      "Epoch 416/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2291\n",
      "Epoch 417/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2291\n",
      "Epoch 418/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2291\n",
      "Epoch 419/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2291\n",
      "Epoch 420/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2291\n",
      "Epoch 421/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2291\n",
      "Epoch 422/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2291\n",
      "Epoch 423/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2291\n",
      "Epoch 424/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2291\n",
      "Epoch 425/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2291\n",
      "Epoch 426/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2291\n",
      "Epoch 427/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2291\n",
      "Epoch 428/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2292\n",
      "Epoch 429/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2294\n",
      "Epoch 430/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2299\n",
      "Epoch 431/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2302\n",
      "Epoch 432/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2301\n",
      "Epoch 433/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2300\n",
      "Epoch 434/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2297\n",
      "Epoch 435/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.2294\n",
      "Epoch 436/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2293\n",
      "Epoch 437/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2296\n",
      "Epoch 438/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2296\n",
      "Epoch 439/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2294\n",
      "Epoch 440/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2293\n",
      "Epoch 441/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2293\n",
      "Epoch 442/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2293\n",
      "Epoch 443/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2293\n",
      "Epoch 444/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2293\n",
      "Epoch 445/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2293\n",
      "Epoch 446/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2292\n",
      "Epoch 447/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2291\n",
      "Epoch 448/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.2292\n",
      "Epoch 449/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2292\n",
      "Epoch 450/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2291\n",
      "Epoch 451/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2291\n",
      "Epoch 452/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2291\n",
      "Epoch 453/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2291\n",
      "Epoch 454/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2291\n",
      "Epoch 455/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2291\n",
      "Epoch 456/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2291\n",
      "Epoch 457/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2291\n",
      "Epoch 458/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.2291\n",
      "Epoch 459/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2291\n",
      "Epoch 460/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2290\n",
      "Epoch 461/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2290\n",
      "Epoch 462/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2290\n",
      "Epoch 463/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2290\n",
      "Epoch 464/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2290\n",
      "Epoch 465/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2290\n",
      "Epoch 466/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2290\n",
      "Epoch 467/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2290\n",
      "Epoch 468/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2290\n",
      "Epoch 469/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2290\n",
      "Epoch 470/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2290\n",
      "Epoch 471/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.2290\n",
      "Epoch 472/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2290\n",
      "Epoch 473/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2290\n",
      "Epoch 474/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2290\n",
      "Epoch 475/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2290\n",
      "Epoch 476/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2290\n",
      "Epoch 477/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2290\n",
      "Epoch 478/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.2290\n",
      "Epoch 479/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2290\n",
      "Epoch 480/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2290\n",
      "Epoch 481/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2290\n",
      "Epoch 482/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2290\n",
      "Epoch 483/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2290\n",
      "Epoch 484/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2290\n",
      "Epoch 485/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2290\n",
      "Epoch 486/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2290\n",
      "Epoch 487/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2290\n",
      "Epoch 488/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.2290\n",
      "Epoch 489/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2290\n",
      "Epoch 490/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2292\n",
      "Epoch 491/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2294\n",
      "Epoch 492/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2298\n",
      "Epoch 493/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2303\n",
      "Epoch 494/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2301\n",
      "Epoch 495/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2295\n",
      "Epoch 496/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2290\n",
      "Epoch 497/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2293\n",
      "Epoch 498/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2295\n",
      "Epoch 499/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2293\n",
      "Epoch 500/500\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2291\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ee72a1dda0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_dataset, epochs=training_epochs,\n",
    "                    steps_per_epoch=dataX.shape[0]//batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the Test Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea."
     ]
    }
   ],
   "source": [
    "results = model.predict(dataX, steps=1)\n",
    "for j, result in enumerate(results):\n",
    "    index = np.argmax(result, axis=1)\n",
    "    if j is 0:  # print all for the first result to make a sentence\n",
    "        print(''.join([idx2char[t] for t in index]), end='')\n",
    "    else:\n",
    "        print(idx2char[index[-1]], end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using GradientTape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((dataX, dataY)).shuffle(\n",
    "                buffer_size=100000).prefetch(buffer_size=batch_size).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 10, 25)            5100      \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 10, 25)            5100      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10, 25)            650       \n",
      "=================================================================\n",
      "Total params: 10,850\n",
      "Trainable params: 10,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def loss_fn(model, dataX, dataY):\n",
    "    logits = model(dataX, training=True)\n",
    "    loss = tf.reduce_mean(keras.losses.categorical_crossentropy(labels, logits))    \n",
    "    return loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train(model, dataX, dataY):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = loss_fn(model, dataX, dataY)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning started. It takes sometime.\n",
      "Epoch: 1 loss = 3.07759881\n",
      "Epoch: 2 loss = 3.06852317\n",
      "Epoch: 3 loss = 2.85521865\n",
      "Epoch: 4 loss = 2.88638926\n",
      "Epoch: 5 loss = 2.85205817\n",
      "Epoch: 6 loss = 2.83029437\n",
      "Epoch: 7 loss = 2.80191469\n",
      "Epoch: 8 loss = 2.77385449\n",
      "Epoch: 9 loss = 2.72482085\n",
      "Epoch: 10 loss = 2.64846969\n",
      "Epoch: 11 loss = 2.58146715\n",
      "Epoch: 12 loss = 2.52702236\n",
      "Epoch: 13 loss = 2.48046231\n",
      "Epoch: 14 loss = 2.36589098\n",
      "Epoch: 15 loss = 2.28911567\n",
      "Epoch: 16 loss = 2.20882058\n",
      "Epoch: 17 loss = 2.10518885\n",
      "Epoch: 18 loss = 2.02794337\n",
      "Epoch: 19 loss = 1.93242681\n",
      "Epoch: 20 loss = 1.82313466\n",
      "Epoch: 21 loss = 1.71803606\n",
      "Epoch: 22 loss = 1.62320411\n",
      "Epoch: 23 loss = 1.52482915\n",
      "Epoch: 24 loss = 1.43298185\n",
      "Epoch: 25 loss = 1.34930778\n",
      "Epoch: 26 loss = 1.25959277\n",
      "Epoch: 27 loss = 1.18905556\n",
      "Epoch: 28 loss = 1.09541285\n",
      "Epoch: 29 loss = 1.03327262\n",
      "Epoch: 30 loss = 0.96477699\n",
      "Epoch: 31 loss = 0.90351033\n",
      "Epoch: 32 loss = 0.83492243\n",
      "Epoch: 33 loss = 0.77805585\n",
      "Epoch: 34 loss = 0.73048568\n",
      "Epoch: 35 loss = 0.67663282\n",
      "Epoch: 36 loss = 0.62261766\n",
      "Epoch: 37 loss = 0.58151484\n",
      "Epoch: 38 loss = 0.54005879\n",
      "Epoch: 39 loss = 0.50420147\n",
      "Epoch: 40 loss = 0.47530946\n",
      "Epoch: 41 loss = 0.44367489\n",
      "Epoch: 42 loss = 0.41758233\n",
      "Epoch: 43 loss = 0.39738953\n",
      "Epoch: 44 loss = 0.37901196\n",
      "Epoch: 45 loss = 0.36287576\n",
      "Epoch: 46 loss = 0.34921944\n",
      "Epoch: 47 loss = 0.33892423\n",
      "Epoch: 48 loss = 0.33656055\n",
      "Epoch: 49 loss = 0.33262998\n",
      "Epoch: 50 loss = 0.32853717\n",
      "Epoch: 51 loss = 0.31360564\n",
      "Epoch: 52 loss = 0.31146580\n",
      "Epoch: 53 loss = 0.30032566\n",
      "Epoch: 54 loss = 0.29530311\n",
      "Epoch: 55 loss = 0.28974634\n",
      "Epoch: 56 loss = 0.28469947\n",
      "Epoch: 57 loss = 0.27935061\n",
      "Epoch: 58 loss = 0.27561757\n",
      "Epoch: 59 loss = 0.27231145\n",
      "Epoch: 60 loss = 0.26921403\n",
      "Epoch: 61 loss = 0.26618883\n",
      "Epoch: 62 loss = 0.26343903\n",
      "Epoch: 63 loss = 0.26173979\n",
      "Epoch: 64 loss = 0.25956160\n",
      "Epoch: 65 loss = 0.25794527\n",
      "Epoch: 66 loss = 0.25576660\n",
      "Epoch: 67 loss = 0.25457203\n",
      "Epoch: 68 loss = 0.25338301\n",
      "Epoch: 69 loss = 0.25184357\n",
      "Epoch: 70 loss = 0.25084162\n",
      "Epoch: 71 loss = 0.25023517\n",
      "Epoch: 72 loss = 0.24964002\n",
      "Epoch: 73 loss = 0.24870268\n",
      "Epoch: 74 loss = 0.24687062\n",
      "Epoch: 75 loss = 0.24694394\n",
      "Epoch: 76 loss = 0.24550588\n",
      "Epoch: 77 loss = 0.24483044\n",
      "Epoch: 78 loss = 0.24504711\n",
      "Epoch: 79 loss = 0.24287632\n",
      "Epoch: 80 loss = 0.24309915\n",
      "Epoch: 81 loss = 0.24210072\n",
      "Epoch: 82 loss = 0.24190803\n",
      "Epoch: 83 loss = 0.24072129\n",
      "Epoch: 84 loss = 0.24064112\n",
      "Epoch: 85 loss = 0.23993878\n",
      "Epoch: 86 loss = 0.24012148\n",
      "Epoch: 87 loss = 0.23885591\n",
      "Epoch: 88 loss = 0.23942581\n",
      "Epoch: 89 loss = 0.23881924\n",
      "Epoch: 90 loss = 0.23819880\n",
      "Epoch: 91 loss = 0.23802888\n",
      "Epoch: 92 loss = 0.23790674\n",
      "Epoch: 93 loss = 0.23713651\n",
      "Epoch: 94 loss = 0.23707937\n",
      "Epoch: 95 loss = 0.23678991\n",
      "Epoch: 96 loss = 0.23632608\n",
      "Epoch: 97 loss = 0.23607817\n",
      "Epoch: 98 loss = 0.23591143\n",
      "Epoch: 99 loss = 0.23545712\n",
      "Epoch: 100 loss = 0.23540321\n",
      "Epoch: 101 loss = 0.23521575\n",
      "Epoch: 102 loss = 0.23487090\n",
      "Epoch: 103 loss = 0.23482771\n",
      "Epoch: 104 loss = 0.23452391\n",
      "Epoch: 105 loss = 0.23433784\n",
      "Epoch: 106 loss = 0.23424342\n",
      "Epoch: 107 loss = 0.23405516\n",
      "Epoch: 108 loss = 0.23385976\n",
      "Epoch: 109 loss = 0.23378265\n",
      "Epoch: 110 loss = 0.23364466\n",
      "Epoch: 111 loss = 0.23342729\n",
      "Epoch: 112 loss = 0.23340574\n",
      "Epoch: 113 loss = 0.23327972\n",
      "Epoch: 114 loss = 0.23309965\n",
      "Epoch: 115 loss = 0.23308328\n",
      "Epoch: 116 loss = 0.23302871\n",
      "Epoch: 117 loss = 0.23298487\n",
      "Epoch: 118 loss = 0.23295788\n",
      "Epoch: 119 loss = 0.23286395\n",
      "Epoch: 120 loss = 0.23275767\n",
      "Epoch: 121 loss = 0.23261273\n",
      "Epoch: 122 loss = 0.23239844\n",
      "Epoch: 123 loss = 0.23229229\n",
      "Epoch: 124 loss = 0.23227029\n",
      "Epoch: 125 loss = 0.23219548\n",
      "Epoch: 126 loss = 0.23214127\n",
      "Epoch: 127 loss = 0.23208924\n",
      "Epoch: 128 loss = 0.23200954\n",
      "Epoch: 129 loss = 0.23190749\n",
      "Epoch: 130 loss = 0.23186141\n",
      "Epoch: 131 loss = 0.23185411\n",
      "Epoch: 132 loss = 0.23177716\n",
      "Epoch: 133 loss = 0.23169357\n",
      "Epoch: 134 loss = 0.23162387\n",
      "Epoch: 135 loss = 0.23159221\n",
      "Epoch: 136 loss = 0.23158923\n",
      "Epoch: 137 loss = 0.23159890\n",
      "Epoch: 138 loss = 0.23161253\n",
      "Epoch: 139 loss = 0.23182283\n",
      "Epoch: 140 loss = 0.23215692\n",
      "Epoch: 141 loss = 0.23293974\n",
      "Epoch: 142 loss = 0.23280771\n",
      "Epoch: 143 loss = 0.23233546\n",
      "Epoch: 144 loss = 0.23136787\n",
      "Epoch: 145 loss = 0.23144993\n",
      "Epoch: 146 loss = 0.23191743\n",
      "Epoch: 147 loss = 0.23167549\n",
      "Epoch: 148 loss = 0.23123725\n",
      "Epoch: 149 loss = 0.23131436\n",
      "Epoch: 150 loss = 0.23160630\n",
      "Epoch: 151 loss = 0.23142575\n",
      "Epoch: 152 loss = 0.23107024\n",
      "Epoch: 153 loss = 0.23121382\n",
      "Epoch: 154 loss = 0.23122519\n",
      "Epoch: 155 loss = 0.23105232\n",
      "Epoch: 156 loss = 0.23091443\n",
      "Epoch: 157 loss = 0.23102850\n",
      "Epoch: 158 loss = 0.23100656\n",
      "Epoch: 159 loss = 0.23079325\n",
      "Epoch: 160 loss = 0.23081757\n",
      "Epoch: 161 loss = 0.23085822\n",
      "Epoch: 162 loss = 0.23077884\n",
      "Epoch: 163 loss = 0.23066805\n",
      "Epoch: 164 loss = 0.23065370\n",
      "Epoch: 165 loss = 0.23072577\n",
      "Epoch: 166 loss = 0.23065472\n",
      "Epoch: 167 loss = 0.23055871\n",
      "Epoch: 168 loss = 0.23057036\n",
      "Epoch: 169 loss = 0.23062378\n",
      "Epoch: 170 loss = 0.23073618\n",
      "Epoch: 171 loss = 0.23091556\n",
      "Epoch: 172 loss = 0.23102045\n",
      "Epoch: 173 loss = 0.23118953\n",
      "Epoch: 174 loss = 0.23115690\n",
      "Epoch: 175 loss = 0.23103185\n",
      "Epoch: 176 loss = 0.23078403\n",
      "Epoch: 177 loss = 0.23066923\n",
      "Epoch: 178 loss = 0.23053375\n",
      "Epoch: 179 loss = 0.23043773\n",
      "Epoch: 180 loss = 0.23042093\n",
      "Epoch: 181 loss = 0.23050316\n",
      "Epoch: 182 loss = 0.23058178\n",
      "Epoch: 183 loss = 0.23044637\n",
      "Epoch: 184 loss = 0.23039062\n",
      "Epoch: 185 loss = 0.23030399\n",
      "Epoch: 186 loss = 0.23024139\n",
      "Epoch: 187 loss = 0.23023392\n",
      "Epoch: 188 loss = 0.23017628\n",
      "Epoch: 189 loss = 0.23019864\n",
      "Epoch: 190 loss = 0.23023279\n",
      "Epoch: 191 loss = 0.23020811\n",
      "Epoch: 192 loss = 0.23019090\n",
      "Epoch: 193 loss = 0.23019026\n",
      "Epoch: 194 loss = 0.23016562\n",
      "Epoch: 195 loss = 0.23016433\n",
      "Epoch: 196 loss = 0.23014666\n",
      "Epoch: 197 loss = 0.23012350\n",
      "Epoch: 198 loss = 0.23012245\n",
      "Epoch: 199 loss = 0.23013085\n",
      "Epoch: 200 loss = 0.23015007\n",
      "Epoch: 201 loss = 0.23022197\n",
      "Epoch: 202 loss = 0.23030561\n",
      "Epoch: 203 loss = 0.23043227\n",
      "Epoch: 204 loss = 0.23054425\n",
      "Epoch: 205 loss = 0.23065282\n",
      "Epoch: 206 loss = 0.23057160\n",
      "Epoch: 207 loss = 0.23034099\n",
      "Epoch: 208 loss = 0.23003083\n",
      "Epoch: 209 loss = 0.22986031\n",
      "Epoch: 210 loss = 0.22989596\n",
      "Epoch: 211 loss = 0.23000100\n",
      "Epoch: 212 loss = 0.23006886\n",
      "Epoch: 213 loss = 0.23003651\n",
      "Epoch: 214 loss = 0.22990888\n",
      "Epoch: 215 loss = 0.22980426\n",
      "Epoch: 216 loss = 0.22979431\n",
      "Epoch: 217 loss = 0.22985163\n",
      "Epoch: 218 loss = 0.22989513\n",
      "Epoch: 219 loss = 0.22986871\n",
      "Epoch: 220 loss = 0.22979602\n",
      "Epoch: 221 loss = 0.22973399\n",
      "Epoch: 222 loss = 0.22973590\n",
      "Epoch: 223 loss = 0.22978534\n",
      "Epoch: 224 loss = 0.22983542\n",
      "Epoch: 225 loss = 0.22986539\n",
      "Epoch: 226 loss = 0.22991735\n",
      "Epoch: 227 loss = 0.22997361\n",
      "Epoch: 228 loss = 0.23011684\n",
      "Epoch: 229 loss = 0.23017474\n",
      "Epoch: 230 loss = 0.23024935\n",
      "Epoch: 231 loss = 0.23007539\n",
      "Epoch: 232 loss = 0.22987270\n",
      "Epoch: 233 loss = 0.22970143\n",
      "Epoch: 234 loss = 0.22965014\n",
      "Epoch: 235 loss = 0.22966982\n",
      "Epoch: 236 loss = 0.22970660\n",
      "Epoch: 237 loss = 0.22971764\n",
      "Epoch: 238 loss = 0.22967027\n",
      "Epoch: 239 loss = 0.22959575\n",
      "Epoch: 240 loss = 0.22957504\n",
      "Epoch: 241 loss = 0.22962768\n",
      "Epoch: 242 loss = 0.22969852\n",
      "Epoch: 243 loss = 0.22976810\n",
      "Epoch: 244 loss = 0.22981578\n",
      "Epoch: 245 loss = 0.22995634\n",
      "Epoch: 246 loss = 0.23015302\n",
      "Epoch: 247 loss = 0.23034115\n",
      "Epoch: 248 loss = 0.23027362\n",
      "Epoch: 249 loss = 0.22993936\n",
      "Epoch: 250 loss = 0.22957602\n",
      "Epoch: 251 loss = 0.22956888\n",
      "Epoch: 252 loss = 0.22974509\n",
      "Epoch: 253 loss = 0.22988780\n",
      "Epoch: 254 loss = 0.22982733\n",
      "Epoch: 255 loss = 0.22959305\n",
      "Epoch: 256 loss = 0.22948672\n",
      "Epoch: 257 loss = 0.22952490\n",
      "Epoch: 258 loss = 0.22964849\n",
      "Epoch: 259 loss = 0.22966298\n",
      "Epoch: 260 loss = 0.22954944\n",
      "Epoch: 261 loss = 0.22945008\n",
      "Epoch: 262 loss = 0.22944221\n",
      "Epoch: 263 loss = 0.22950806\n",
      "Epoch: 264 loss = 0.22953244\n",
      "Epoch: 265 loss = 0.22948845\n",
      "Epoch: 266 loss = 0.22941510\n",
      "Epoch: 267 loss = 0.22938554\n",
      "Epoch: 268 loss = 0.22940065\n",
      "Epoch: 269 loss = 0.22943358\n",
      "Epoch: 270 loss = 0.22943644\n",
      "Epoch: 271 loss = 0.22941218\n",
      "Epoch: 272 loss = 0.22938181\n",
      "Epoch: 273 loss = 0.22937186\n",
      "Epoch: 274 loss = 0.22939926\n",
      "Epoch: 275 loss = 0.22947448\n",
      "Epoch: 276 loss = 0.22962536\n",
      "Epoch: 277 loss = 0.22988763\n",
      "Epoch: 278 loss = 0.23027767\n",
      "Epoch: 279 loss = 0.23050292\n",
      "Epoch: 280 loss = 0.23024310\n",
      "Epoch: 281 loss = 0.22962698\n",
      "Epoch: 282 loss = 0.22935951\n",
      "Epoch: 283 loss = 0.22962721\n",
      "Epoch: 284 loss = 0.22986145\n",
      "Epoch: 285 loss = 0.22955070\n",
      "Epoch: 286 loss = 0.22934292\n",
      "Epoch: 287 loss = 0.22955577\n",
      "Epoch: 288 loss = 0.22960919\n",
      "Epoch: 289 loss = 0.22941470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 290 loss = 0.22939564\n",
      "Epoch: 291 loss = 0.22957191\n",
      "Epoch: 292 loss = 0.22955830\n",
      "Epoch: 293 loss = 0.22948480\n",
      "Epoch: 294 loss = 0.22970267\n",
      "Epoch: 295 loss = 0.22990137\n",
      "Epoch: 296 loss = 0.22993030\n",
      "Epoch: 297 loss = 0.23008181\n",
      "Epoch: 298 loss = 0.23008305\n",
      "Epoch: 299 loss = 0.22974060\n",
      "Epoch: 300 loss = 0.22946411\n",
      "Epoch: 301 loss = 0.22938548\n",
      "Epoch: 302 loss = 0.22936256\n",
      "Epoch: 303 loss = 0.22949858\n",
      "Epoch: 304 loss = 0.22964962\n",
      "Epoch: 305 loss = 0.22958548\n",
      "Epoch: 306 loss = 0.22941774\n",
      "Epoch: 307 loss = 0.22932495\n",
      "Epoch: 308 loss = 0.22930190\n",
      "Epoch: 309 loss = 0.22931379\n",
      "Epoch: 310 loss = 0.22935778\n",
      "Epoch: 311 loss = 0.22937642\n",
      "Epoch: 312 loss = 0.22930758\n",
      "Epoch: 313 loss = 0.22921930\n",
      "Epoch: 314 loss = 0.22922346\n",
      "Epoch: 315 loss = 0.22927731\n",
      "Epoch: 316 loss = 0.22928859\n",
      "Epoch: 317 loss = 0.22925732\n",
      "Epoch: 318 loss = 0.22921351\n",
      "Epoch: 319 loss = 0.22918625\n",
      "Epoch: 320 loss = 0.22918084\n",
      "Epoch: 321 loss = 0.22919035\n",
      "Epoch: 322 loss = 0.22920689\n",
      "Epoch: 323 loss = 0.22921298\n",
      "Epoch: 324 loss = 0.22920187\n",
      "Epoch: 325 loss = 0.22916380\n",
      "Epoch: 326 loss = 0.22913788\n",
      "Epoch: 327 loss = 0.22913311\n",
      "Epoch: 328 loss = 0.22913809\n",
      "Epoch: 329 loss = 0.22914396\n",
      "Epoch: 330 loss = 0.22915184\n",
      "Epoch: 331 loss = 0.22915728\n",
      "Epoch: 332 loss = 0.22915924\n",
      "Epoch: 333 loss = 0.22916634\n",
      "Epoch: 334 loss = 0.22917771\n",
      "Epoch: 335 loss = 0.22919089\n",
      "Epoch: 336 loss = 0.22920667\n",
      "Epoch: 337 loss = 0.22923261\n",
      "Epoch: 338 loss = 0.22927222\n",
      "Epoch: 339 loss = 0.22933963\n",
      "Epoch: 340 loss = 0.22952138\n",
      "Epoch: 341 loss = 0.22997789\n",
      "Epoch: 342 loss = 0.23042355\n",
      "Epoch: 343 loss = 0.22988500\n",
      "Epoch: 344 loss = 0.23021239\n",
      "Epoch: 345 loss = 0.22974001\n",
      "Epoch: 346 loss = 0.22984539\n",
      "Epoch: 347 loss = 0.22982655\n",
      "Epoch: 348 loss = 0.22972877\n",
      "Epoch: 349 loss = 0.22975641\n",
      "Epoch: 350 loss = 0.22961958\n",
      "Epoch: 351 loss = 0.22952472\n",
      "Epoch: 352 loss = 0.22951052\n",
      "Epoch: 353 loss = 0.22947888\n",
      "Epoch: 354 loss = 0.22949940\n",
      "Epoch: 355 loss = 0.22951467\n",
      "Epoch: 356 loss = 0.22939001\n",
      "Epoch: 357 loss = 0.22933151\n",
      "Epoch: 358 loss = 0.22926012\n",
      "Epoch: 359 loss = 0.22924341\n",
      "Epoch: 360 loss = 0.22923420\n",
      "Epoch: 361 loss = 0.22920373\n",
      "Epoch: 362 loss = 0.22918479\n",
      "Epoch: 363 loss = 0.22917455\n",
      "Epoch: 364 loss = 0.22915545\n",
      "Epoch: 365 loss = 0.22914574\n",
      "Epoch: 366 loss = 0.22914501\n",
      "Epoch: 367 loss = 0.22914565\n",
      "Epoch: 368 loss = 0.22913525\n",
      "Epoch: 369 loss = 0.22915328\n",
      "Epoch: 370 loss = 0.22919983\n",
      "Epoch: 371 loss = 0.22931188\n",
      "Epoch: 372 loss = 0.22947976\n",
      "Epoch: 373 loss = 0.22975194\n",
      "Epoch: 374 loss = 0.22995861\n",
      "Epoch: 375 loss = 0.22983058\n",
      "Epoch: 376 loss = 0.22945495\n",
      "Epoch: 377 loss = 0.22921216\n",
      "Epoch: 378 loss = 0.22921211\n",
      "Epoch: 379 loss = 0.22934636\n",
      "Epoch: 380 loss = 0.22939108\n",
      "Epoch: 381 loss = 0.22933067\n",
      "Epoch: 382 loss = 0.22919515\n",
      "Epoch: 383 loss = 0.22914056\n",
      "Epoch: 384 loss = 0.22917424\n",
      "Epoch: 385 loss = 0.22921506\n",
      "Epoch: 386 loss = 0.22917783\n",
      "Epoch: 387 loss = 0.22908784\n",
      "Epoch: 388 loss = 0.22906072\n",
      "Epoch: 389 loss = 0.22910020\n",
      "Epoch: 390 loss = 0.22913657\n",
      "Epoch: 391 loss = 0.22911581\n",
      "Epoch: 392 loss = 0.22904448\n",
      "Epoch: 393 loss = 0.22901559\n",
      "Epoch: 394 loss = 0.22904433\n",
      "Epoch: 395 loss = 0.22908503\n",
      "Epoch: 396 loss = 0.22908983\n",
      "Epoch: 397 loss = 0.22903870\n",
      "Epoch: 398 loss = 0.22901392\n",
      "Epoch: 399 loss = 0.22902282\n",
      "Epoch: 400 loss = 0.22908074\n",
      "Epoch: 401 loss = 0.22914594\n",
      "Epoch: 402 loss = 0.22922701\n",
      "Epoch: 403 loss = 0.22927742\n",
      "Epoch: 404 loss = 0.22936766\n",
      "Epoch: 405 loss = 0.22923285\n",
      "Epoch: 406 loss = 0.22915410\n",
      "Epoch: 407 loss = 0.22909276\n",
      "Epoch: 408 loss = 0.22904217\n",
      "Epoch: 409 loss = 0.22903197\n",
      "Epoch: 410 loss = 0.22903776\n",
      "Epoch: 411 loss = 0.22903323\n",
      "Epoch: 412 loss = 0.22902037\n",
      "Epoch: 413 loss = 0.22900990\n",
      "Epoch: 414 loss = 0.22902220\n",
      "Epoch: 415 loss = 0.22904545\n",
      "Epoch: 416 loss = 0.22906066\n",
      "Epoch: 417 loss = 0.22905126\n",
      "Epoch: 418 loss = 0.22902998\n",
      "Epoch: 419 loss = 0.22901125\n",
      "Epoch: 420 loss = 0.22900495\n",
      "Epoch: 421 loss = 0.22900322\n",
      "Epoch: 422 loss = 0.22899993\n",
      "Epoch: 423 loss = 0.22898956\n",
      "Epoch: 424 loss = 0.22897929\n",
      "Epoch: 425 loss = 0.22897351\n",
      "Epoch: 426 loss = 0.22897710\n",
      "Epoch: 427 loss = 0.22899134\n",
      "Epoch: 428 loss = 0.22901899\n",
      "Epoch: 429 loss = 0.22904462\n",
      "Epoch: 430 loss = 0.22908443\n",
      "Epoch: 431 loss = 0.22912265\n",
      "Epoch: 432 loss = 0.22918935\n",
      "Epoch: 433 loss = 0.22925407\n",
      "Epoch: 434 loss = 0.22934674\n",
      "Epoch: 435 loss = 0.22941568\n",
      "Epoch: 436 loss = 0.22945699\n",
      "Epoch: 437 loss = 0.22943930\n",
      "Epoch: 438 loss = 0.22932595\n",
      "Epoch: 439 loss = 0.22922565\n",
      "Epoch: 440 loss = 0.22917089\n",
      "Epoch: 441 loss = 0.22924741\n",
      "Epoch: 442 loss = 0.22944289\n",
      "Epoch: 443 loss = 0.22958006\n",
      "Epoch: 444 loss = 0.22962976\n",
      "Epoch: 445 loss = 0.22940619\n",
      "Epoch: 446 loss = 0.22920553\n",
      "Epoch: 447 loss = 0.22909616\n",
      "Epoch: 448 loss = 0.22910859\n",
      "Epoch: 449 loss = 0.22911946\n",
      "Epoch: 450 loss = 0.22905417\n",
      "Epoch: 451 loss = 0.22899392\n",
      "Epoch: 452 loss = 0.22900978\n",
      "Epoch: 453 loss = 0.22905521\n",
      "Epoch: 454 loss = 0.22903347\n",
      "Epoch: 455 loss = 0.22894308\n",
      "Epoch: 456 loss = 0.22891276\n",
      "Epoch: 457 loss = 0.22896543\n",
      "Epoch: 458 loss = 0.22901407\n",
      "Epoch: 459 loss = 0.22898248\n",
      "Epoch: 460 loss = 0.22891569\n",
      "Epoch: 461 loss = 0.22889079\n",
      "Epoch: 462 loss = 0.22890894\n",
      "Epoch: 463 loss = 0.22892436\n",
      "Epoch: 464 loss = 0.22892396\n",
      "Epoch: 465 loss = 0.22891025\n",
      "Epoch: 466 loss = 0.22889797\n",
      "Epoch: 467 loss = 0.22888649\n",
      "Epoch: 468 loss = 0.22887442\n",
      "Epoch: 469 loss = 0.22887069\n",
      "Epoch: 470 loss = 0.22887567\n",
      "Epoch: 471 loss = 0.22888733\n",
      "Epoch: 472 loss = 0.22889805\n",
      "Epoch: 473 loss = 0.22890033\n",
      "Epoch: 474 loss = 0.22889993\n",
      "Epoch: 475 loss = 0.22890770\n",
      "Epoch: 476 loss = 0.22893636\n",
      "Epoch: 477 loss = 0.22902715\n",
      "Epoch: 478 loss = 0.22909288\n",
      "Epoch: 479 loss = 0.22928691\n",
      "Epoch: 480 loss = 0.22928818\n",
      "Epoch: 481 loss = 0.22935361\n",
      "Epoch: 482 loss = 0.22927929\n",
      "Epoch: 483 loss = 0.22913291\n",
      "Epoch: 484 loss = 0.22901179\n",
      "Epoch: 485 loss = 0.22896451\n",
      "Epoch: 486 loss = 0.22902058\n",
      "Epoch: 487 loss = 0.22913124\n",
      "Epoch: 488 loss = 0.22927038\n",
      "Epoch: 489 loss = 0.22938161\n",
      "Epoch: 490 loss = 0.22949362\n",
      "Epoch: 491 loss = 0.22945979\n",
      "Epoch: 492 loss = 0.22926621\n",
      "Epoch: 493 loss = 0.22903612\n",
      "Epoch: 494 loss = 0.22897208\n",
      "Epoch: 495 loss = 0.22905138\n",
      "Epoch: 496 loss = 0.22911766\n",
      "Epoch: 497 loss = 0.22905758\n",
      "Epoch: 498 loss = 0.22896862\n",
      "Epoch: 499 loss = 0.22896081\n",
      "Epoch: 500 loss = 0.22897786\n",
      "Learning Finished!\n"
     ]
    }
   ],
   "source": [
    "# train my model\n",
    "print('Learning started. It takes sometime.')\n",
    "for epoch in range(training_epochs):\n",
    "    avg_loss = 0.    \n",
    "    train_step = 0    \n",
    "    \n",
    "    for images, labels in train_dataset:\n",
    "        train(model,images, labels)\n",
    "        loss = loss_fn(model, images, labels)        \n",
    "        avg_loss = avg_loss + loss        \n",
    "        train_step += 1\n",
    "    avg_loss = avg_loss / train_step    \n",
    "\n",
    "    print('Epoch:', '{}'.format(epoch + 1), 'loss =', '{:.8f}'.format(avg_loss))\n",
    "\n",
    "\n",
    "print('Learning Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea."
     ]
    }
   ],
   "source": [
    "results = model(dataX, training=False)\n",
    "\n",
    "for j, result in enumerate(results):\n",
    "    index = np.argmax(result, axis=1)\n",
    "    if j is 0:  # print all for the first result to make a sentence\n",
    "        print(''.join([idx2char[t] for t in index]), end='')\n",
    "    else:\n",
    "        print(idx2char[index[-1]], end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
